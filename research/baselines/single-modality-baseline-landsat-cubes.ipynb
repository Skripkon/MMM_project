{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006251,
     "end_time": "2024-05-05T21:00:58.823056",
     "exception": false,
     "start_time": "2024-05-05T21:00:58.816805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Simple baseline with Landsat Cubes â€” ResNet6 + Binary Cross Entropy\n",
    "\n",
    "To demonstrate the potential of a single-modality baseline with Landsat cubes, we provide a straightforward baseline based on a custom ResNet6-like architecture and Binary Cross-Entropy. The model itself should learn the relationship between the location's [R, G, B, NIR, SWIR1, and SWIR2] values and its species composition.\n",
    "\n",
    "Considering the significant extent of enhancing the performance of this baseline, we encourage you to experiment with various techniques, architectures, losses, etc.\n",
    "\n",
    "#### **Have Fun!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/23048869/Desktop/untitled folder/MMM_project\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:07.29831Z",
     "start_time": "2024-04-30T21:25:05.354584Z"
    },
    "execution": {
     "iopub.execute_input": "2025-09-04T18:31:52.659311Z",
     "iopub.status.busy": "2025-09-04T18:31:52.659116Z",
     "iopub.status.idle": "2025-09-04T18:32:06.113163Z",
     "shell.execute_reply": "2025-09-04T18:32:06.112490Z",
     "shell.execute_reply.started": "2025-09-04T18:31:52.659293Z"
    },
    "papermill": {
     "duration": 7.910804,
     "end_time": "2024-05-05T21:01:06.739690",
     "exception": false,
     "start_time": "2024-05-05T21:00:58.828886",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T13:30:07.054038Z",
     "iopub.status.busy": "2024-05-01T13:30:07.053659Z",
     "iopub.status.idle": "2024-05-01T13:30:07.058148Z",
     "shell.execute_reply": "2024-05-01T13:30:07.057269Z",
     "shell.execute_reply.started": "2024-05-01T13:30:07.054008Z"
    },
    "papermill": {
     "duration": 0.005438,
     "end_time": "2024-05-05T21:01:06.751219",
     "exception": false,
     "start_time": "2024-05-05T21:01:06.745781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data description\n",
    "\n",
    "Satellite time series data includes over 20 years of Landsat satellite imagery extracted from [Ecodatacube](https://stac.ecodatacube.eu/).\n",
    "The data was acquired through the Landsat satellite program and pre-processed by Ecodatacube to produce raster files scaled to the entire European continent and projected into a unique CRS.\n",
    "\n",
    "Since the original rasters require a high amount of disk space, we extracted the data points from each spectral band corresponding to all PA locations (i.e., GPS coordinates) and aggregated them in data cubes as tensor objects. Each data point corresponds to the mean value of Landsat's observations at the given location for three months before the given time. The cubes are structured as follows.\n",
    "**Shape**: `(n_bands, n_quarters, n_years)` where:\n",
    "- `n_bands` = 6 comprising [`red`, `green`, `blue`, `nir`, `swir1`, `swir2`]\n",
    "- `n_quarters` = 4 \n",
    "    - *Quarter 1*: December 2 of previous year until March 20 of current year (winter season proxy),\n",
    "    - *Quarter 2*: March 21 until June 24 of current year (spring season proxy),\n",
    "    - *Quarter 3*: June 25 until September 12 of current year (summer season proxy),\n",
    "    - *Quarter 4*: September 13 until December 1 of current year (fall season proxy).\n",
    "- `n_years` = 21 (ranging from 2000 to 2020)\n",
    "\n",
    "The datacubes can simply be loaded as tensors using PyTorch with the following command :\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.load('path_to_file.pt')\n",
    "```\n",
    "\n",
    "**References:**\n",
    "- *Traceability (lineage): This dataset is a seasonally aggregated and gapfilled version of the Landsat GLAD analysis-ready data product presented by Potapov et al., 2020 ( https://doi.org/10.3390/rs12030426 ).*\n",
    "- *Scientific methodology: The Landsat GLAD ARD dataset was aggregated and harmonized using the eumap python package (available at https://eumap.readthedocs.io/en/latest/ ). The full process of gapfilling and harmonization is described in detail in Witjes et al., 2022 (in review, preprint available at https://doi.org/10.21203/rs.3.rs-561383/v3 ).*\n",
    "- *Ecodatacube.eu: Analysis-ready open environmental data cube for Europe (https://doi.org/10.21203/rs.3.rs-2277090/v3).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005422,
     "end_time": "2024-05-05T21:01:06.762205",
     "exception": false,
     "start_time": "2024-05-05T21:01:06.756783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare custom dataset loader\n",
    "\n",
    "We have to sloightly update the Dataset to provide the relevant data in the appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:32.627928Z",
     "start_time": "2024-04-30T21:25:32.612131Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-04T18:32:06.114817Z",
     "iopub.status.busy": "2025-09-04T18:32:06.114474Z",
     "iopub.status.idle": "2025-09-04T18:32:06.123956Z",
     "shell.execute_reply": "2025-09-04T18:32:06.123257Z",
     "shell.execute_reply.started": "2025-09-04T18:32:06.114798Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022488,
     "end_time": "2024-05-05T21:01:06.790318",
     "exception": false,
     "start_time": "2024-05-05T21:01:06.767830",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        sample = torch.nan_to_num(torch.load(os.path.join(self.data_dir, f\"GLC25-PA-{self.subset}-landsat-time-series_{survey_id}_cube.pt\"), weights_only=True))\n",
    "\n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(11255)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "\n",
    "        # Ensure the sample is in the correct format for the transform\n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            sample = sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            sample = sample.numpy()  # Convert tensor to numpy array\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label, survey_id\n",
    "    \n",
    "class TestDataset(TrainDataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        sample = torch.nan_to_num(torch.load(os.path.join(self.data_dir, f\"GLC25-PA-{self.subset}-landsat_time_series_{survey_id}_cube.pt\"), weights_only=True))\n",
    "\n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            sample = sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            sample = sample.numpy()\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, survey_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00538,
     "end_time": "2024-05-05T21:01:06.801283",
     "exception": false,
     "start_time": "2024-05-05T21:01:06.795903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load metadata and prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:34.532017Z",
     "start_time": "2024-04-30T21:25:32.615562Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-04T18:32:06.124891Z",
     "iopub.status.busy": "2025-09-04T18:32:06.124662Z",
     "iopub.status.idle": "2025-09-04T18:32:10.301971Z",
     "shell.execute_reply": "2025-09-04T18:32:10.301269Z",
     "shell.execute_reply.started": "2025-09-04T18:32:06.124870Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.726171,
     "end_time": "2024-05-05T21:01:12.535279",
     "exception": false,
     "start_time": "2024-05-05T21:01:06.809108",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load Training metadata\n",
    "\n",
    "\n",
    "train_data_path = \"data/SateliteTimeSeries-Landsat/cubes/PA-train/\"\n",
    "train_metadata_path = \"data/GLC25_PA_metadata_train.csv\"\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "train_dataset = TrainDataset(train_data_path, train_metadata, subset=\"train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Load Test metadata\n",
    "test_data_path = \"data/SateliteTimeSeries-Landsat/cubes/PA-test/\"\n",
    "test_metadata_path = \"data/GLC25_PA_metadata_test.csv\"\n",
    "test_metadata = pd.read_csv(test_metadata_path)\n",
    "test_dataset = TestDataset(test_data_path, test_metadata, subset=\"test\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005747,
     "end_time": "2024-05-05T21:01:12.547063",
     "exception": false,
     "start_time": "2024-05-05T21:01:12.541316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define and initialize the ModifiedResNet18 model\n",
    "\n",
    "To utilize the landsat cubes, which have a shape of [6,4,21] (BANDs, QUARTERs, and YEARs), some minor adjustments must be made to the vanilla ResNet-18. It's important to note that this is just one method for ensuring compatibility with the unusual tensor shape, and experimentation is encouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.014067Z",
     "start_time": "2024-04-30T21:25:31.01006Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-04T18:32:10.303060Z",
     "iopub.status.busy": "2025-09-04T18:32:10.302793Z",
     "iopub.status.idle": "2025-09-04T18:32:10.319628Z",
     "shell.execute_reply": "2025-09-04T18:32:10.318976Z",
     "shell.execute_reply.started": "2025-09-04T18:32:10.303041Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015915,
     "end_time": "2024-05-05T21:01:12.568601",
     "exception": false,
     "start_time": "2024-05-05T21:01:12.552686",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"2Ã—(Conv3Ã—3 + BN + ReLU) with identity skip; stride=1 (no downsample).\"\"\"\n",
    "    def __init__(self, c: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(c, c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(c)\n",
    "        self.conv2 = nn.Conv2d(c, c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x); out = self.bn1(out); out = F.relu(out, inplace=True)\n",
    "        out = self.conv2(out); out = self.bn2(out)\n",
    "        out = out + identity\n",
    "        out = F.relu(out, inplace=True)\n",
    "        return out\n",
    "\n",
    "class ResNet6(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-6 for Landsat cubes:\n",
    "      - Input: [B, 6, 4, 21]  (bands, quarters, years)\n",
    "      - Stem: 1Ã—1 conv (channel mixing) + 3Ã—3 conv (spatial-temporal)\n",
    "      - 3 residual blocks (6 conv layers total)\n",
    "      - GAP + MLP head -> logits for multi-label BCEWithLogitsLoss\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, stem_channels: int = 64, mlp_hidden: int = 512, p_drop: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Per-sample normalization over [C,H,W]\n",
    "        self.norm_input = nn.LayerNorm([6, 4, 21])\n",
    "\n",
    "        # Two-step stem to (i) mix spectral bands, then (ii) capture local 2D structure\n",
    "        self.stem1 = nn.Conv2d(6, stem_channels, kernel_size=1, stride=1, padding=0, bias=False)  # channel mixing\n",
    "        self.stem2 = nn.Conv2d(stem_channels, stem_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.stem_bn = nn.BatchNorm2d(stem_channels)\n",
    "\n",
    "        # 3 residual blocks (no downsampling; preserve 4Ã—21)\n",
    "        self.block1 = BasicBlock(stem_channels)\n",
    "        self.block2 = BasicBlock(stem_channels)\n",
    "        self.block3 = BasicBlock(stem_channels)\n",
    "\n",
    "        # Global average pooling and head\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),                       # [B, C, 1, 1] -> [B, C]\n",
    "            nn.LayerNorm(stem_channels),\n",
    "            nn.Linear(stem_channels, mlp_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(mlp_hidden, num_classes)  # logits\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 6, 4, 21]\n",
    "        x = self.norm_input(x)\n",
    "        x = self.stem1(x)                  # [B, C=stem_channels, 4, 21]\n",
    "        x = self.stem2(x)\n",
    "        x = self.stem_bn(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "\n",
    "        x = self.block1(x)                 # 3 blocks Ã— 2 conv = 6 conv layers\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        x = self.gap(x)                    # [B, C, 1, 1]\n",
    "        x = self.head(x)                   # [B, num_classes] (logits)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T18:32:10.320684Z",
     "iopub.status.busy": "2025-09-04T18:32:10.320387Z",
     "iopub.status.idle": "2025-09-04T18:32:10.445099Z",
     "shell.execute_reply": "2025-09-04T18:32:10.444318Z",
     "shell.execute_reply.started": "2025-09-04T18:32:10.320652Z"
    },
    "papermill": {
     "duration": 0.058735,
     "end_time": "2024-05-05T21:01:12.632872",
     "exception": false,
     "start_time": "2024-05-05T21:01:12.574137",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set seed for Python's built-in random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # Set seed for numpy\n",
    "    np.random.seed(seed)\n",
    "    # Set seed for CUDA if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Set cuDNN's random number generator seed for deterministic behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.611823Z",
     "start_time": "2024-04-30T21:25:31.607373Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-04T18:32:10.446296Z",
     "iopub.status.busy": "2025-09-04T18:32:10.446023Z",
     "iopub.status.idle": "2025-09-04T18:32:10.816066Z",
     "shell.execute_reply": "2025-09-04T18:32:10.815517Z",
     "shell.execute_reply.started": "2025-09-04T18:32:10.446270Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.676292,
     "end_time": "2024-05-05T21:01:13.314798",
     "exception": false,
     "start_time": "2024-05-05T21:01:12.638506",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = MPS\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"DEVICE = MPS\")\n",
    "\n",
    "\n",
    "num_classes = 11255 # Number of all unique classes within the PO and PA data.\n",
    "model = ResNet6(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005509,
     "end_time": "2024-05-05T21:01:13.326480",
     "exception": false,
     "start_time": "2024-05-05T21:01:13.320971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Loop\n",
    "\n",
    "Nothing special, just a standard Pytorch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:32.181927Z",
     "start_time": "2024-04-30T21:25:32.177073Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-04T18:32:10.818090Z",
     "iopub.status.busy": "2025-09-04T18:32:10.817875Z",
     "iopub.status.idle": "2025-09-04T18:32:10.824866Z",
     "shell.execute_reply": "2025-09-04T18:32:10.824326Z",
     "shell.execute_reply.started": "2025-09-04T18:32:10.818074Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015096,
     "end_time": "2024-05-05T21:01:13.347192",
     "exception": false,
     "start_time": "2024-05-05T21:01:13.332096",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsymmetricLoss(nn.Module):\n",
    "    \"\"\"Asymmetric Loss for multi-label classification\"\"\"\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
    "\n",
    "        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n",
    "        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))\n",
    "        loss = los_pos + los_neg\n",
    "\n",
    "        pt0 = xs_pos * y\n",
    "        pt1 = xs_neg * (1 - y)\n",
    "        pt = pt0 + pt1\n",
    "        one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n",
    "        one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
    "        loss *= one_sided_w\n",
    "\n",
    "        return -loss.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T21:25:34.536634Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-04T18:32:10.825808Z",
     "iopub.status.busy": "2025-09-04T18:32:10.825593Z",
     "iopub.status.idle": "2025-09-04T18:41:32.504762Z",
     "shell.execute_reply": "2025-09-04T18:41:32.503851Z",
     "shell.execute_reply.started": "2025-09-04T18:32:10.825784Z"
    },
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1209.843817,
     "end_time": "2024-05-05T21:21:23.196800",
     "exception": false,
     "start_time": "2024-05-05T21:01:13.352983",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 epochs started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   1%|          | 11/1391 [00:00<00:52, 26.42it/s, batch_loss=8.3186, avg_loss=8.2007]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs started.\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch_idx, (data, targets, _) in pbar:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "\n",
    "        # criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        criterion = AsymmetricLoss()\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "        avg_loss = running_loss / (batch_idx + 1)\n",
    "\n",
    "        # Show loss in the tqdm bar\n",
    "        pbar.set_postfix({\n",
    "            \"batch_loss\": f\"{loss.item():.4f}\",\n",
    "            \"avg_loss\": f\"{avg_loss:.4f}\"\n",
    "        })\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"\\nEpoch {epoch+1} finished in {epoch_time:.2f} seconds\")\n",
    "    print(\"Scheduler:\", scheduler.state_dict())\n",
    "\n",
    "# Save the trained model\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), \"resnet6-with-landsat-cubes.pth\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training completed in {total_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014707,
     "end_time": "2024-05-05T21:21:23.226855",
     "exception": false,
     "start_time": "2024-05-05T21:21:23.212148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Loop\n",
    "\n",
    "Again, nothing special, just a standard inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-04T18:41:32.506063Z",
     "iopub.status.busy": "2025-09-04T18:41:32.505809Z",
     "iopub.status.idle": "2025-09-04T18:41:57.747301Z",
     "shell.execute_reply": "2025-09-04T18:41:57.746387Z",
     "shell.execute_reply.started": "2025-09-04T18:41:32.506037Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 9.799329,
     "end_time": "2024-05-05T21:21:33.041149",
     "exception": false,
     "start_time": "2024-05-05T21:21:23.241820",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:13<00:00, 16.99it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    surveys = []\n",
    "    top_k_indices = None\n",
    "    for data, surveyID in tqdm(test_loader, total=len(test_loader)):\n",
    "\n",
    "        data = data.to(device)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "        # Sellect top-25 values as predictions\n",
    "        top_25 = np.argsort(-predictions, axis=1)[:, :25] \n",
    "        if top_k_indices is None:\n",
    "            top_k_indices = top_25\n",
    "        else:\n",
    "            top_k_indices = np.concatenate((top_k_indices, top_25), axis=0)\n",
    "\n",
    "        surveys.extend(surveyID.cpu().numpy())\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     all_predictions = []\n",
    "#     surveys = []\n",
    "#     top_k_indices = []\n",
    "#     for data, surveyID in tqdm(test_loader, total=len(test_loader)):\n",
    "\n",
    "#         data = data.to(device)\n",
    "        \n",
    "#         outputs = model(data)\n",
    "#         predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "#         # Sellect top-25 values as predictions\n",
    "#         top_25 = np.argsort(-predictions, axis=1)[:, :25] \n",
    "#         if top_k_indices is None:\n",
    "#             top_k_indices = top_25\n",
    "#         else:\n",
    "#             top_k_indices = np.concatenate((top_k_indices, top_25), axis=0)\n",
    "\n",
    "#         # indices_above_0_5_per_sample = [np.where(sample > 0.1)[0] for sample in predictions]\n",
    "#         # top_k_indices.extend(indices_above_0_5_per_sample)\n",
    "\n",
    "#         surveys.extend(surveyID.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01651,
     "end_time": "2024-05-05T21:21:33.076908",
     "exception": false,
     "start_time": "2024-05-05T21:21:33.060398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save prediction file! ðŸŽ‰ðŸ¥³ðŸ™ŒðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T18:41:57.748678Z",
     "iopub.status.busy": "2025-09-04T18:41:57.748436Z",
     "iopub.status.idle": "2025-09-04T18:41:57.937272Z",
     "shell.execute_reply": "2025-09-04T18:41:57.936748Z",
     "shell.execute_reply.started": "2025-09-04T18:41:57.748647Z"
    },
    "papermill": {
     "duration": 0.124413,
     "end_time": "2024-05-05T21:21:33.218031",
     "exception": false,
     "start_time": "2024-05-05T21:21:33.093618",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_concatenated = [' '.join(map(str, row)) for row in top_k_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'surveyId': surveys,\n",
    "     'predictions': data_concatenated,\n",
    "    }).to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median 25.0\n",
      "Mean 25.0\n",
      "Max 25\n",
      "Min 25\n"
     ]
    }
   ],
   "source": [
    "lengths = [\n",
    "    len(x.split()) for x in data_concatenated\n",
    "]\n",
    "\n",
    "print(\"Median\", np.median(lengths))\n",
    "print(\"Mean\", np.mean(lengths))\n",
    "print(\"Max\", np.max(lengths))\n",
    "print(\"Min\", np.min(lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13622514,
     "sourceId": 114201,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1239.673699,
   "end_time": "2024-05-05T21:21:35.811865",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-05T21:00:56.138166",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
