{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0de5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71263415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.dataset_utils import (\n",
    "    read_satellite_image,\n",
    "    get_bioclimatic_time_series_cube,\n",
    "    get_satellite_time_series_landsat_cube,\n",
    "    read_environmental_values,\n",
    "    get_environmental_values_tensor\n",
    ")\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "train_data = pd.read_csv(\"data/GLC25_PA_metadata_train.csv\")\n",
    "test_data = pd.read_csv(\"data/GLC25_PA_metadata_test.csv\")\n",
    "\n",
    "def preprocessing(data):\n",
    "\n",
    "    environmental_data = read_environmental_values()\n",
    "    environmental_data = environmental_data.drop(columns=[\"surveyId\"])\n",
    "    # change NaN and inf to median of the column\n",
    "    for column in environmental_data.columns:\n",
    "        median = environmental_data[column].median()\n",
    "        environmental_data.loc[environmental_data[column].isna(), column] = median\n",
    "        environmental_data.loc[environmental_data[column] == np.inf, column] = median\n",
    "\n",
    "    # normalize each column of the environmental_data dataframe\n",
    "    for column in environmental_data.columns:\n",
    "        environmental_data[column] = (environmental_data[column] - environmental_data[column].mean()) / environmental_data[column].std()\n",
    "\n",
    "    table_data_meta_columns = ['lon', 'lat', 'year', 'geoUncertaintyInM', 'areaInM2', 'region', 'country']\n",
    "    def get_text(row):\n",
    "        text = \"\"\n",
    "        for column in table_data_meta_columns:\n",
    "            text += f\"{column}: {row[column]}\\n\"\n",
    "        return text\n",
    "\n",
    "    data[\"text\"] = data.apply(get_text, axis=1)\n",
    "    return environmental_data, data\n",
    "\n",
    "environmental_train_data, train_data = preprocessing(train_data)\n",
    "environmental_test_data, test_data = preprocessing(test_data)\n",
    "train_data[\"speciesId\"] = train_data[\"speciesId\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e08211",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unique_texts = list(train_data.text.unique())\n",
    "test_unique_texts = list(test_data.text.unique())\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\", local_files_only=True).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\", local_files_only=True, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a6e7e",
   "metadata": {},
   "source": [
    "# Get text embeddings via CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd37d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(unique_texts, split: str, batch_inference_size = 128):\n",
    "\n",
    "    fp_save_path = f\"data/{split}_text_embeddings.pt\"\n",
    "\n",
    "    if not os.path.exists(fp_save_path):\n",
    "        text_embeddings = []\n",
    "        for i in tqdm(range(0, len(unique_texts), batch_inference_size)):\n",
    "            batch_texts = unique_texts[i:i+batch_inference_size]\n",
    "            inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            text_features = model.get_text_features(**inputs)\n",
    "            text_embeddings.append(text_features.detach().cpu())\n",
    "\n",
    "        embeddings_torch = torch.cat(text_embeddings, dim=0)\n",
    "        torch.save(embeddings_torch, fp_save_path)\n",
    "\n",
    "    else:\n",
    "        embeddings_torch = torch.load(fp_save_path)\n",
    "    return embeddings_torch\n",
    "\n",
    "train_text_embeddings = get_embeddings(train_unique_texts, \"train\")\n",
    "test_text_embeddings = get_embeddings(test_unique_texts, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38380de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each surveyId get a list of speciesIds\n",
    "survey_id2species_ids = train_data.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "\n",
    "species_ids = train_data.speciesId.unique()\n",
    "species_id2label = {species_id.item(): i for i, species_id in enumerate(species_ids)}\n",
    "label2species_id = {i: species_id.item() for i, species_id in enumerate(species_ids)}\n",
    "\n",
    "n_classes = len(species_ids)\n",
    "n_classes  # 5016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data.speciesId.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13297257",
   "metadata": {},
   "source": [
    "# Get image embeddings via CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_embeddings = []\n",
    "# for survey_id in tqdm(train_data.surveyId.unique()): # TEST MODE\n",
    "#     image = read_satellite_image(survey_id) \n",
    "#     inputs = processor(images=[image[:3, :, :]], return_tensors=\"pt\")\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#     outputs = model.get_image_features(**inputs)\n",
    "#     image_embeddings.append(outputs.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "188a6d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14784/14784 [00:51<00:00, 284.56it/s]\n",
      "100%|██████████| 88987/88987 [04:08<00:00, 358.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Task: multi-label prediction\n",
    "# Input:  (4, 64, 64) + (4, 19, 12) + (6, 4, 21) + (65,)\n",
    "# Output: (n_classes,)\n",
    "# Loss: BCEwithLogitsLoss + assymetric loss\n",
    "\n",
    "# CLIP   -> v1 (512, )  -| cross_attention(CLIP, LSTM) -> (512, )  Q=CLIP, K=LSTM, V=LSTM\n",
    "# LSTM 1 -> v2 (512, )  -| cross attention(CLIP, LSTM) -> (512, )  Q=LSTM, K=CLIP, V=CLIP\n",
    "# LSTM 2 -> v3 (512, )\n",
    "# MLP    -> v4 (512, )\n",
    "\n",
    "# v1 | v2 | v3 | v4 -> (2048, )\n",
    "# MLP 2048 -> (num_classes,)\n",
    "\n",
    "def get_X_y(data, environmental_values, text_embeddings, split_folder):\n",
    "    unique_survey_ids = data.surveyId.unique()\n",
    "\n",
    "    # survey_id -> geo -> [labels]\n",
    "    X_all, y_all = [], []\n",
    "\n",
    "    for idx, survey_id in tqdm(enumerate(unique_survey_ids), total=len(unique_survey_ids)): # TEST MODE\n",
    "\n",
    "        X, y = [], []\n",
    "        # IMAGE --> CNN / ViT on the first 3 channels + custom CNN on the last channel (NIR)\n",
    "        image = read_satellite_image(survey_id, split_folder)                                       # shape: (4, 64, 64)  [torch.int16]\n",
    "\n",
    "        # TIME SERIES --> LSTM\n",
    "        # 4 variables\n",
    "        # 19 years, 12 months\n",
    "        # 4 times series: 19 * 12 = 228\n",
    "        bioclimatic_time_series_cube = get_bioclimatic_time_series_cube(survey_id, split_folder)                   # shape: (4, 19, 12)  [torch.float32]\n",
    "\n",
    "        # TIME SERIES --> LSTM  (but there are some RGB channels involved -- maybe it should be treated as a picture)\n",
    "        # 6 variables\n",
    "        # 4 = four times per year (after each season)\n",
    "        # 21 years\n",
    "        # 6 time series: 4 * 21 = 84\n",
    "        satellite_time_series_landsat_cube = get_satellite_time_series_landsat_cube(survey_id, split_folder)       # shape: (6, 4, 21)   [torch.float32]\n",
    "        # set all nan and inf to 0\n",
    "        satellite_time_series_landsat_cube[satellite_time_series_landsat_cube != satellite_time_series_landsat_cube] = 0\n",
    "\n",
    "        # MLP ???\n",
    "        # 65 variables\n",
    "        # 1 variable\n",
    "        # 65 time series: 1 * 65 = 65\n",
    "        # environmental_values = get_environmental_values_tensor(survey_id, environmental_data)  # shape: (1, 65)      [torch.float64]\n",
    "\n",
    "        X = {\n",
    "            \"satellite_image_RGB\": image[:3, :, :],\n",
    "            \"satellite_image_NIR\": torch.tensor(image[3, :, :]),  # shape (1, 64, 64)\n",
    "\n",
    "            \"bioclimatic_time_series_cube\": bioclimatic_time_series_cube.reshape(4 * 19, -1),\n",
    "            \"satellite_time_series_landsat_cube\": satellite_time_series_landsat_cube.reshape(6 * 4, -1),\n",
    "            \"environmental_values\": torch.tensor(environmental_values[idx], dtype=torch.float32),\n",
    "            \"text_embedding\": text_embeddings[idx]\n",
    "        }\n",
    "        X_all.append(X)\n",
    "\n",
    "        if split_folder == \"PA-train\":\n",
    "            y = survey_id2species_ids[survey_id]\n",
    "            y_all.append(y)\n",
    "\n",
    "    return X_all, y_all\n",
    "\n",
    "X_test, _ = get_X_y(test_data, environmental_test_data.values, test_text_embeddings, \"PA-test\")\n",
    "X_train, y_train = get_X_y(train_data, environmental_train_data.values, train_text_embeddings, \"PA-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "feb7f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize bioclimatic_time_series_cube: this is 4 time series each of 19 * 12 = 228 values\n",
    "def normalize_bioclimatic_time_series(X_all):\n",
    "    # Stack all cubes for joint normalization\n",
    "    cubes = [x[\"bioclimatic_time_series_cube\"] for x in X_all]\n",
    "    cubes_stacked = torch.stack(cubes)  # shape: (num_samples, 4, 228)\n",
    "    # Compute mean and std over all samples, per-channel (over axis 0)\n",
    "    mean = cubes_stacked.mean(dim=0, keepdim=True)  # shape: (1, 4, 228)\n",
    "    std = cubes_stacked.std(dim=0, keepdim=True) + 1e-8  # to avoid div by zero\n",
    "    # Apply normalization\n",
    "    for idx, x in enumerate(X_all):\n",
    "        x[\"bioclimatic_time_series_cube\"] = (x[\"bioclimatic_time_series_cube\"] - mean.squeeze(0)) / std.squeeze(0)\n",
    "    return X_all\n",
    "\n",
    "X_train = normalize_bioclimatic_time_series(X_train)\n",
    "X_test = normalize_bioclimatic_time_series(X_test)\n",
    "\n",
    "def normalize_satellite_time_series(X_all):\n",
    "    cubes = [x[\"satellite_time_series_landsat_cube\"] for x in X_all]\n",
    "    cubes_stacked = torch.stack(cubes)  # shape: (num_samples, 6 * 4, 21)\n",
    "\n",
    "    mean = cubes_stacked.mean(dim=0, keepdim=True)  # shape: (1, 6 * 4, 21)\n",
    "    std = cubes_stacked.std(dim=0, keepdim=True) + 1e-8  # to avoid div by zero\n",
    "    for idx, x in enumerate(X_all):\n",
    "        x[\"satellite_time_series_landsat_cube\"] = (x[\"satellite_time_series_landsat_cube\"] - mean.squeeze(0)) / std.squeeze(0)\n",
    "    return X_all\n",
    "\n",
    "X_train = normalize_satellite_time_series(X_train)\n",
    "X_test = normalize_satellite_time_series(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47a5d0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88987/88987 [00:07<00:00, 11822.56it/s]\n"
     ]
    }
   ],
   "source": [
    "y_train_multi_hot_encoded = []\n",
    "\n",
    "for y_sample in tqdm(y_train):\n",
    "    y_sample_class_labels = [\n",
    "        species_id2label[l] for l in y_sample\n",
    "    ]\n",
    "    multi_hot_encoded = [0 for _ in range(n_classes)]\n",
    "    for l in y_sample_class_labels:\n",
    "        multi_hot_encoded[l] = 1\n",
    "    \n",
    "    y_train_multi_hot_encoded.append(multi_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "678d2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_features=32):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Accept variable input channels for CNN\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 8, kernel_size=3, stride=2, padding=1),   # (batch, 8, 32, 32)\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),  # (batch, 16, 16, 16)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # (batch, 32, 8, 8)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))                           # (batch, 32, 1, 1)\n",
    "        )\n",
    "        self.out_features = out_features\n",
    "        self.projector = nn.Linear(32, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(dtype=torch.float32)\n",
    "        # Accept only (batch, C, H, W), do not squeeze channels unless necessary.\n",
    "        # If x is (batch, H, W), unsqueeze to get 1 channel\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.projector(x)\n",
    "        return x\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.bioclim_lstm_input_size = 12  # Each time series is of length 12\n",
    "        self.bioclim_lstm_hidden_size = 32  # Reduced hidden size\n",
    "        self.bioclim_lstm_num_layers = 4\n",
    "\n",
    "        # Smaller LSTM for the bioclimatic time series\n",
    "        self.bioclim_lstm = nn.LSTM(\n",
    "            input_size=self.bioclim_lstm_input_size,\n",
    "            hidden_size=self.bioclim_lstm_hidden_size,\n",
    "            num_layers=self.bioclim_lstm_num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.satellite_lstm_input_size = 21 \n",
    "        self.satellite_lstm_hidden_size = 32\n",
    "        self.satellite_lstm_num_layers = 2\n",
    "\n",
    "        self.satellite_lstm = nn.LSTM(\n",
    "            input_size=self.satellite_lstm_input_size,\n",
    "            hidden_size=self.satellite_lstm_hidden_size,\n",
    "            num_layers=self.satellite_lstm_num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.text_embeddings_projector = nn.Linear(768, 32)\n",
    "        self.environmental_values_projector = nn.Linear(64, 32)\n",
    "\n",
    "        # Now we will use CNN output in combined features too\n",
    "        self.combined_features = nn.Linear(32 + 32 + self.bioclim_lstm_hidden_size + self.satellite_lstm_hidden_size + 32, 64)\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "\n",
    "        # Determine the correct number of channels for satellite image input at runtime in forward\n",
    "        self.satellite_image_cnn = None  # Lazy initialization in forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        text_embeddings = torch.stack([\n",
    "            x_i[\"text_embedding\"] for x_i in x\n",
    "        ]).to(device, dtype=torch.float32)\n",
    "        text_embeddings_projected = self.text_embeddings_projector(text_embeddings)\n",
    "\n",
    "        environmental_values = torch.stack([\n",
    "            x_i[\"environmental_values\"] for x_i in x\n",
    "        ]).to(device, dtype=torch.float32)\n",
    "        environmental_values_projected = self.environmental_values_projector(environmental_values)\n",
    "\n",
    "        bioclimatic_time_series_cube = torch.stack([\n",
    "            x_i[\"bioclimatic_time_series_cube\"]\n",
    "            for x_i in x\n",
    "        ]).to(device, dtype=torch.float32)  # shape: (batch_size, 4, 228)\n",
    "\n",
    "        satellite_time_series_landsat_cube = torch.stack([\n",
    "            x_i[\"satellite_time_series_landsat_cube\"]\n",
    "            for x_i in x\n",
    "        ]).to(device, dtype=torch.float32)  # shape: (batch_size, 6 * 4, 21)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, (h_n, c_n) = self.satellite_lstm(satellite_time_series_landsat_cube)\n",
    "        satellite_lstm_features = lstm_out[:, -1, :]  # shape (batch, satellite_lstm_hidden_size)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, (h_n, c_n) = self.bioclim_lstm(bioclimatic_time_series_cube)\n",
    "        bioclim_lstm_features = h_n[-1]  # shape (batch, bioclim_lstm_hidden_size)\n",
    "\n",
    "        # Stack all satellite images. Accept multiple channels if present.\n",
    "        satellite_images = torch.stack([\n",
    "            x_i[\"satellite_image_NIR\"]\n",
    "            for x_i in x\n",
    "        ]).to(device, dtype=torch.float32)  # shape: (batch_size, C, 64, 64) or (batch_size, 1, 64, 64)\n",
    "        # If satellite_images is (batch, 64, 64), add channel dim\n",
    "        if satellite_images.dim() == 3:\n",
    "            satellite_images = satellite_images.unsqueeze(1)\n",
    "        in_channels = satellite_images.size(1)\n",
    "\n",
    "        # Lazy initialize CNN to handle correct input channels\n",
    "        if (self.satellite_image_cnn is None) or \\\n",
    "           (hasattr(self.satellite_image_cnn, 'cnn') and getattr(self.satellite_image_cnn.cnn[0], 'in_channels', 1) != in_channels):\n",
    "            self.satellite_image_cnn = SimpleCNN(in_channels=in_channels, out_features=32).to(device)\n",
    "\n",
    "        satellite_image_features = self.satellite_image_cnn(satellite_images)  # shape: (batch_size, 32)\n",
    "\n",
    "        # Concatenate features with CNN features included\n",
    "        concat = torch.cat([\n",
    "                text_embeddings_projected, \n",
    "                environmental_values_projected,\n",
    "                satellite_lstm_features, \n",
    "                bioclim_lstm_features,\n",
    "                satellite_image_features\n",
    "            ], dim=1)\n",
    "        features = self.combined_features(concat)\n",
    "        logits = self.classifier(features)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4b6cef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion = AsymmetricLoss()\n",
    "mmm = MultiModalModel(num_classes=len(species_ids)).to(device)\n",
    "optimizer = Adam(mmm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e01f3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split, y_train_multi_hot_encoded_split, y_train_split = X_train[:88_000], y_train_multi_hot_encoded[:88_000], y_train[:88_000]\n",
    "X_val_split, y_val_multi_hot_encoded_split, y_val_split = X_train[88_000:], y_train_multi_hot_encoded[88_000:], y_train[88_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "058c0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL\n",
    "def make_predictions(torch_model, subset):\n",
    "\n",
    "    test_inference_batch_size = 256\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        n_test = len(subset)\n",
    "        for step in range(0, n_test, test_inference_batch_size):\n",
    "            batch_X = subset[step : step + test_inference_batch_size]\n",
    "            batch_logits = torch_model(batch_X)\n",
    "            batch_probs = torch.sigmoid(batch_logits)\n",
    "            # take top 25 by probability\n",
    "            # batch_preds = [\n",
    "            #     torch.topk(probs, 25).indices.tolist()\n",
    "            #     for probs in batch_probs\n",
    "            # ]\n",
    "            batch_preds = [\n",
    "                torch.where(probs > 0.1)[0].tolist() or torch.topk(probs, 25).indices.tolist()\n",
    "                for probs in batch_probs\n",
    "            ]\n",
    "            predictions.extend(batch_preds)\n",
    "\n",
    "    for idx, p in enumerate(predictions):\n",
    "        predictions[idx] = [\n",
    "            label2species_id[label] for label in p\n",
    "        ]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def eval_haccard(predictions, targets):\n",
    "    score = 0\n",
    "    for pred, ground_truth in zip(predictions, targets):\n",
    "        score += len(set(pred) & set(ground_truth)) / len(set(pred) | set(ground_truth))\n",
    "    score /= len(predictions)\n",
    "    return score\n",
    "\n",
    "def eval_f1_micro(predictions, targets):\n",
    "    # predictions[i] is a list of labels\n",
    "    # targets[i] is a list of labels\n",
    "    # we need to compute the TP, FP, FN for each pair (pred, target) and then compute the f1 micro score\n",
    "    TP = []\n",
    "    FP = []\n",
    "    FN = []\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        TP.append(len(set(pred) & set(target)))\n",
    "        FP.append(len(set(pred) - set(target)))\n",
    "        FN.append(len(set(target) - set(pred)))\n",
    "    return 1 / len(predictions) * sum(TP[i] / (TP[i] + (FP[i] + FN[i]) / 2) for i in range(len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a64cc141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 1/695 | Loss: 0.6948239207267761\n",
      "Train HACCARD: 0.0031851076555024565 | Train F1 Micro: 0.006342824775877274 | Val HACCARD: 0.0032164550641594716 | Val F1 Micro: 0.0064050267009411535\n",
      "Epoch 1 | Step 2/695 | Loss: 0.6913779377937317\n",
      "Epoch 1 | Step 3/695 | Loss: 0.6859808564186096\n",
      "Epoch 1 | Step 4/695 | Loss: 0.678429365158081\n",
      "Epoch 1 | Step 5/695 | Loss: 0.6673648953437805\n",
      "Epoch 1 | Step 6/695 | Loss: 0.652824878692627\n",
      "Epoch 1 | Step 7/695 | Loss: 0.634341299533844\n",
      "Epoch 1 | Step 8/695 | Loss: 0.6099648475646973\n",
      "Epoch 1 | Step 9/695 | Loss: 0.5807234048843384\n",
      "Epoch 1 | Step 10/695 | Loss: 0.5469526052474976\n",
      "Epoch 1 | Step 11/695 | Loss: 0.5094784498214722\n",
      "Epoch 1 | Step 12/695 | Loss: 0.467572420835495\n",
      "Epoch 1 | Step 13/695 | Loss: 0.42099037766456604\n",
      "Epoch 1 | Step 14/695 | Loss: 0.37070780992507935\n",
      "Epoch 1 | Step 15/695 | Loss: 0.32141199707984924\n",
      "Epoch 1 | Step 16/695 | Loss: 0.2745509743690491\n",
      "Epoch 1 | Step 17/695 | Loss: 0.22781406342983246\n",
      "Epoch 1 | Step 18/695 | Loss: 0.18775129318237305\n",
      "Epoch 1 | Step 19/695 | Loss: 0.15059000253677368\n",
      "Epoch 1 | Step 20/695 | Loss: 0.11896199733018875\n",
      "Epoch 1 | Step 21/695 | Loss: 0.09444992244243622\n",
      "Epoch 1 | Step 22/695 | Loss: 0.07419401407241821\n",
      "Epoch 1 | Step 23/695 | Loss: 0.06076430529356003\n",
      "Epoch 1 | Step 24/695 | Loss: 0.04925866425037384\n",
      "Epoch 1 | Step 25/695 | Loss: 0.04041815176606178\n",
      "Epoch 1 | Step 26/695 | Loss: 0.03353393077850342\n",
      "Epoch 1 | Step 27/695 | Loss: 0.030244985595345497\n",
      "Epoch 1 | Step 28/695 | Loss: 0.026647862046957016\n",
      "Epoch 1 | Step 29/695 | Loss: 0.027218854054808617\n",
      "Epoch 1 | Step 30/695 | Loss: 0.02445748634636402\n",
      "Epoch 1 | Step 31/695 | Loss: 0.029296064749360085\n",
      "Epoch 1 | Step 32/695 | Loss: 0.026036178693175316\n",
      "Epoch 1 | Step 33/695 | Loss: 0.023153722286224365\n",
      "Epoch 1 | Step 34/695 | Loss: 0.023455718532204628\n",
      "Epoch 1 | Step 35/695 | Loss: 0.023105522617697716\n",
      "Epoch 1 | Step 36/695 | Loss: 0.023843295872211456\n",
      "Epoch 1 | Step 37/695 | Loss: 0.023268701508641243\n",
      "Epoch 1 | Step 38/695 | Loss: 0.021418195217847824\n",
      "Epoch 1 | Step 39/695 | Loss: 0.02017415314912796\n",
      "Epoch 1 | Step 40/695 | Loss: 0.02312752604484558\n",
      "Epoch 1 | Step 41/695 | Loss: 0.019737277179956436\n",
      "Epoch 1 | Step 42/695 | Loss: 0.02146870456635952\n",
      "Epoch 1 | Step 43/695 | Loss: 0.020875485613942146\n",
      "Epoch 1 | Step 44/695 | Loss: 0.019183697178959846\n",
      "Epoch 1 | Step 45/695 | Loss: 0.019016044214367867\n",
      "Epoch 1 | Step 46/695 | Loss: 0.019929789006710052\n",
      "Epoch 1 | Step 47/695 | Loss: 0.01993604376912117\n",
      "Epoch 1 | Step 48/695 | Loss: 0.019813448190689087\n",
      "Epoch 1 | Step 49/695 | Loss: 0.01905841752886772\n",
      "Epoch 1 | Step 50/695 | Loss: 0.020319459959864616\n",
      "Epoch 1 | Step 51/695 | Loss: 0.017075752839446068\n",
      "Epoch 1 | Step 52/695 | Loss: 0.019789213314652443\n",
      "Epoch 1 | Step 53/695 | Loss: 0.019461631774902344\n",
      "Epoch 1 | Step 54/695 | Loss: 0.018183305859565735\n",
      "Epoch 1 | Step 55/695 | Loss: 0.0168041680008173\n",
      "Epoch 1 | Step 56/695 | Loss: 0.017826493829488754\n",
      "Epoch 1 | Step 57/695 | Loss: 0.019243743270635605\n",
      "Epoch 1 | Step 58/695 | Loss: 0.020241325721144676\n",
      "Epoch 1 | Step 59/695 | Loss: 0.01849045418202877\n",
      "Epoch 1 | Step 60/695 | Loss: 0.01619764044880867\n",
      "Epoch 1 | Step 61/695 | Loss: 0.0173451267182827\n",
      "Epoch 1 | Step 62/695 | Loss: 0.015347539447247982\n",
      "Epoch 1 | Step 63/695 | Loss: 0.015739135444164276\n",
      "Epoch 1 | Step 64/695 | Loss: 0.019229264929890633\n",
      "Epoch 1 | Step 65/695 | Loss: 0.015981927514076233\n",
      "Epoch 1 | Step 66/695 | Loss: 0.017569860443472862\n",
      "Epoch 1 | Step 67/695 | Loss: 0.016532020643353462\n",
      "Epoch 1 | Step 68/695 | Loss: 0.01595657877624035\n",
      "Epoch 1 | Step 69/695 | Loss: 0.015961434692144394\n",
      "Epoch 1 | Step 70/695 | Loss: 0.015003800392150879\n",
      "Epoch 1 | Step 71/695 | Loss: 0.015712488442659378\n",
      "Epoch 1 | Step 72/695 | Loss: 0.0160208772867918\n",
      "Epoch 1 | Step 73/695 | Loss: 0.019289745017886162\n",
      "Epoch 1 | Step 74/695 | Loss: 0.01638498157262802\n",
      "Epoch 1 | Step 75/695 | Loss: 0.014987954869866371\n",
      "Epoch 1 | Step 76/695 | Loss: 0.016678405925631523\n",
      "Epoch 1 | Step 77/695 | Loss: 0.018027251586318016\n",
      "Epoch 1 | Step 78/695 | Loss: 0.016124894842505455\n",
      "Epoch 1 | Step 79/695 | Loss: 0.014550809748470783\n",
      "Epoch 1 | Step 80/695 | Loss: 0.016137778759002686\n",
      "Epoch 1 | Step 81/695 | Loss: 0.016417058184742928\n",
      "Epoch 1 | Step 82/695 | Loss: 0.015458578243851662\n",
      "Epoch 1 | Step 83/695 | Loss: 0.014745837077498436\n",
      "Epoch 1 | Step 84/695 | Loss: 0.01584099791944027\n",
      "Epoch 1 | Step 85/695 | Loss: 0.015241947025060654\n",
      "Epoch 1 | Step 86/695 | Loss: 0.015238611027598381\n",
      "Epoch 1 | Step 87/695 | Loss: 0.014087053015828133\n",
      "Epoch 1 | Step 88/695 | Loss: 0.015385976992547512\n",
      "Epoch 1 | Step 89/695 | Loss: 0.014299141243100166\n",
      "Epoch 1 | Step 90/695 | Loss: 0.01498592272400856\n",
      "Epoch 1 | Step 91/695 | Loss: 0.015224242582917213\n",
      "Epoch 1 | Step 92/695 | Loss: 0.015509572811424732\n",
      "Epoch 1 | Step 93/695 | Loss: 0.014228759333491325\n",
      "Epoch 1 | Step 94/695 | Loss: 0.01583431474864483\n",
      "Epoch 1 | Step 95/695 | Loss: 0.014703178778290749\n",
      "Epoch 1 | Step 96/695 | Loss: 0.016004400327801704\n",
      "Epoch 1 | Step 97/695 | Loss: 0.01584676094353199\n",
      "Epoch 1 | Step 98/695 | Loss: 0.014289339073002338\n",
      "Epoch 1 | Step 99/695 | Loss: 0.01558765023946762\n",
      "Epoch 1 | Step 100/695 | Loss: 0.013663138262927532\n",
      "Epoch 1 | Step 101/695 | Loss: 0.015550825744867325\n",
      "Epoch 1 | Step 102/695 | Loss: 0.013709585182368755\n",
      "Epoch 1 | Step 103/695 | Loss: 0.014648865908384323\n",
      "Epoch 1 | Step 104/695 | Loss: 0.014549676328897476\n",
      "Epoch 1 | Step 105/695 | Loss: 0.01544683426618576\n",
      "Epoch 1 | Step 106/695 | Loss: 0.014512072317302227\n",
      "Epoch 1 | Step 107/695 | Loss: 0.013765547424554825\n",
      "Epoch 1 | Step 108/695 | Loss: 0.014268415980041027\n",
      "Epoch 1 | Step 109/695 | Loss: 0.015330822207033634\n",
      "Epoch 1 | Step 110/695 | Loss: 0.015346572734415531\n",
      "Epoch 1 | Step 111/695 | Loss: 0.014742877334356308\n",
      "Epoch 1 | Step 112/695 | Loss: 0.016562215983867645\n",
      "Epoch 1 | Step 113/695 | Loss: 0.014887085184454918\n",
      "Epoch 1 | Step 114/695 | Loss: 0.01529265008866787\n",
      "Epoch 1 | Step 115/695 | Loss: 0.014191029593348503\n",
      "Epoch 1 | Step 116/695 | Loss: 0.015491694211959839\n",
      "Epoch 1 | Step 117/695 | Loss: 0.014331435784697533\n",
      "Epoch 1 | Step 118/695 | Loss: 0.014650225639343262\n",
      "Epoch 1 | Step 119/695 | Loss: 0.014695199206471443\n",
      "Epoch 1 | Step 120/695 | Loss: 0.013827869668602943\n",
      "Epoch 1 | Step 121/695 | Loss: 0.015441952273249626\n",
      "Epoch 1 | Step 122/695 | Loss: 0.016191847622394562\n",
      "Epoch 1 | Step 123/695 | Loss: 0.01487309392541647\n",
      "Epoch 1 | Step 124/695 | Loss: 0.013976046815514565\n",
      "Epoch 1 | Step 125/695 | Loss: 0.01610439456999302\n",
      "Epoch 1 | Step 126/695 | Loss: 0.015723073855042458\n",
      "Epoch 1 | Step 127/695 | Loss: 0.015301968902349472\n",
      "Epoch 1 | Step 128/695 | Loss: 0.014110472053289413\n",
      "Epoch 1 | Step 129/695 | Loss: 0.014450617134571075\n",
      "Epoch 1 | Step 130/695 | Loss: 0.01659330539405346\n",
      "Epoch 1 | Step 131/695 | Loss: 0.014732026495039463\n",
      "Epoch 1 | Step 132/695 | Loss: 0.01466335728764534\n",
      "Epoch 1 | Step 133/695 | Loss: 0.015211760997772217\n",
      "Epoch 1 | Step 134/695 | Loss: 0.01474669948220253\n",
      "Epoch 1 | Step 135/695 | Loss: 0.01434359885752201\n",
      "Epoch 1 | Step 136/695 | Loss: 0.01471499353647232\n",
      "Epoch 1 | Step 137/695 | Loss: 0.015447830781340599\n",
      "Epoch 1 | Step 138/695 | Loss: 0.015168718062341213\n",
      "Epoch 1 | Step 139/695 | Loss: 0.013948796316981316\n",
      "Epoch 1 | Step 140/695 | Loss: 0.01423710398375988\n",
      "Epoch 1 | Step 141/695 | Loss: 0.01432336401194334\n",
      "Epoch 1 | Step 142/695 | Loss: 0.015018000267446041\n",
      "Epoch 1 | Step 143/695 | Loss: 0.01485287956893444\n",
      "Epoch 1 | Step 144/695 | Loss: 0.013994417153298855\n",
      "Epoch 1 | Step 145/695 | Loss: 0.013885680586099625\n",
      "Epoch 1 | Step 146/695 | Loss: 0.01483898051083088\n",
      "Epoch 1 | Step 147/695 | Loss: 0.014138918370008469\n",
      "Epoch 1 | Step 148/695 | Loss: 0.01388146448880434\n",
      "Epoch 1 | Step 149/695 | Loss: 0.015187770128250122\n",
      "Epoch 1 | Step 150/695 | Loss: 0.01454172283411026\n",
      "Epoch 1 | Step 151/695 | Loss: 0.014479166828095913\n",
      "Epoch 1 | Step 152/695 | Loss: 0.013606138527393341\n",
      "Epoch 1 | Step 153/695 | Loss: 0.014272010885179043\n",
      "Epoch 1 | Step 154/695 | Loss: 0.014662593603134155\n",
      "Epoch 1 | Step 155/695 | Loss: 0.01599366031587124\n",
      "Epoch 1 | Step 156/695 | Loss: 0.014159498736262321\n",
      "Epoch 1 | Step 157/695 | Loss: 0.01569456234574318\n",
      "Epoch 1 | Step 158/695 | Loss: 0.01519750151783228\n",
      "Epoch 1 | Step 159/695 | Loss: 0.015552009455859661\n",
      "Epoch 1 | Step 160/695 | Loss: 0.013588002882897854\n",
      "Epoch 1 | Step 161/695 | Loss: 0.014008640311658382\n",
      "Epoch 1 | Step 162/695 | Loss: 0.015248863026499748\n",
      "Epoch 1 | Step 163/695 | Loss: 0.014261981472373009\n",
      "Epoch 1 | Step 164/695 | Loss: 0.014060446061193943\n",
      "Epoch 1 | Step 165/695 | Loss: 0.015139426104724407\n",
      "Epoch 1 | Step 166/695 | Loss: 0.014306042343378067\n",
      "Epoch 1 | Step 167/695 | Loss: 0.014380207285284996\n",
      "Epoch 1 | Step 168/695 | Loss: 0.014284246601164341\n",
      "Epoch 1 | Step 169/695 | Loss: 0.014749176800251007\n",
      "Epoch 1 | Step 170/695 | Loss: 0.014482458122074604\n",
      "Epoch 1 | Step 171/695 | Loss: 0.01464415155351162\n",
      "Epoch 1 | Step 172/695 | Loss: 0.01578356698155403\n",
      "Epoch 1 | Step 173/695 | Loss: 0.014851010404527187\n",
      "Epoch 1 | Step 174/695 | Loss: 0.014980416744947433\n",
      "Epoch 1 | Step 175/695 | Loss: 0.015295375138521194\n",
      "Epoch 1 | Step 176/695 | Loss: 0.014291391707956791\n",
      "Epoch 1 | Step 177/695 | Loss: 0.014242391102015972\n",
      "Epoch 1 | Step 178/695 | Loss: 0.014248596504330635\n",
      "Epoch 1 | Step 179/695 | Loss: 0.013555467128753662\n",
      "Epoch 1 | Step 180/695 | Loss: 0.013928812928497791\n",
      "Epoch 1 | Step 181/695 | Loss: 0.015128924511373043\n",
      "Epoch 1 | Step 182/695 | Loss: 0.013508335687220097\n",
      "Epoch 1 | Step 183/695 | Loss: 0.015285604633390903\n",
      "Epoch 1 | Step 184/695 | Loss: 0.014408357441425323\n",
      "Epoch 1 | Step 185/695 | Loss: 0.013989708386361599\n",
      "Epoch 1 | Step 186/695 | Loss: 0.013479010201990604\n",
      "Epoch 1 | Step 187/695 | Loss: 0.01546214334666729\n",
      "Epoch 1 | Step 188/695 | Loss: 0.013769607059657574\n",
      "Epoch 1 | Step 189/695 | Loss: 0.015247314237058163\n",
      "Epoch 1 | Step 190/695 | Loss: 0.014799457043409348\n",
      "Epoch 1 | Step 191/695 | Loss: 0.013678479008376598\n",
      "Epoch 1 | Step 192/695 | Loss: 0.013983594253659248\n",
      "Epoch 1 | Step 193/695 | Loss: 0.014530862681567669\n",
      "Epoch 1 | Step 194/695 | Loss: 0.011983639560639858\n",
      "Epoch 1 | Step 195/695 | Loss: 0.013439697213470936\n",
      "Epoch 1 | Step 196/695 | Loss: 0.01440892368555069\n",
      "Epoch 1 | Step 197/695 | Loss: 0.014820411801338196\n",
      "Epoch 1 | Step 198/695 | Loss: 0.01425967924296856\n",
      "Epoch 1 | Step 199/695 | Loss: 0.014089838601648808\n",
      "Epoch 1 | Step 200/695 | Loss: 0.015073765069246292\n",
      "Epoch 1 | Step 201/695 | Loss: 0.015194496139883995\n",
      "Epoch 1 | Step 202/695 | Loss: 0.01469877827912569\n",
      "Epoch 1 | Step 203/695 | Loss: 0.014697243459522724\n",
      "Epoch 1 | Step 204/695 | Loss: 0.016571875661611557\n",
      "Epoch 1 | Step 205/695 | Loss: 0.014516782015562057\n",
      "Epoch 1 | Step 206/695 | Loss: 0.013706685043871403\n",
      "Epoch 1 | Step 207/695 | Loss: 0.014785509556531906\n",
      "Epoch 1 | Step 208/695 | Loss: 0.014647770673036575\n",
      "Epoch 1 | Step 209/695 | Loss: 0.015067238360643387\n",
      "Epoch 1 | Step 210/695 | Loss: 0.01397196389734745\n",
      "Epoch 1 | Step 211/695 | Loss: 0.01584620587527752\n",
      "Epoch 1 | Step 212/695 | Loss: 0.014879315160214901\n",
      "Epoch 1 | Step 213/695 | Loss: 0.01463908888399601\n",
      "Epoch 1 | Step 214/695 | Loss: 0.014277362264692783\n",
      "Epoch 1 | Step 215/695 | Loss: 0.013809065334498882\n",
      "Epoch 1 | Step 216/695 | Loss: 0.014213557355105877\n",
      "Epoch 1 | Step 217/695 | Loss: 0.015331652946770191\n",
      "Epoch 1 | Step 218/695 | Loss: 0.014430279843509197\n",
      "Epoch 1 | Step 219/695 | Loss: 0.014623836614191532\n",
      "Epoch 1 | Step 220/695 | Loss: 0.012804240919649601\n",
      "Epoch 1 | Step 221/695 | Loss: 0.013247889466583729\n",
      "Epoch 1 | Step 222/695 | Loss: 0.014675323851406574\n",
      "Epoch 1 | Step 223/695 | Loss: 0.013908430002629757\n",
      "Epoch 1 | Step 224/695 | Loss: 0.013592958450317383\n",
      "Epoch 1 | Step 225/695 | Loss: 0.012794927693903446\n",
      "Epoch 1 | Step 226/695 | Loss: 0.014024225994944572\n",
      "Epoch 1 | Step 227/695 | Loss: 0.014339166693389416\n",
      "Epoch 1 | Step 228/695 | Loss: 0.014133633114397526\n",
      "Epoch 1 | Step 229/695 | Loss: 0.012951105833053589\n",
      "Epoch 1 | Step 230/695 | Loss: 0.014589727856218815\n",
      "Epoch 1 | Step 231/695 | Loss: 0.014548360370099545\n",
      "Epoch 1 | Step 232/695 | Loss: 0.012789656408131123\n",
      "Epoch 1 | Step 233/695 | Loss: 0.013745917938649654\n",
      "Epoch 1 | Step 234/695 | Loss: 0.014326050877571106\n",
      "Epoch 1 | Step 235/695 | Loss: 0.014874663203954697\n",
      "Epoch 1 | Step 236/695 | Loss: 0.013833310455083847\n",
      "Epoch 1 | Step 237/695 | Loss: 0.01393841952085495\n",
      "Epoch 1 | Step 238/695 | Loss: 0.013344988226890564\n",
      "Epoch 1 | Step 239/695 | Loss: 0.01267294678837061\n",
      "Epoch 1 | Step 240/695 | Loss: 0.013426406309008598\n",
      "Epoch 1 | Step 241/695 | Loss: 0.012600528076291084\n",
      "Epoch 1 | Step 242/695 | Loss: 0.013832615688443184\n",
      "Epoch 1 | Step 243/695 | Loss: 0.01368382852524519\n",
      "Epoch 1 | Step 244/695 | Loss: 0.013972712680697441\n",
      "Epoch 1 | Step 245/695 | Loss: 0.013280147686600685\n",
      "Epoch 1 | Step 246/695 | Loss: 0.014135384932160378\n",
      "Epoch 1 | Step 247/695 | Loss: 0.013718768022954464\n",
      "Epoch 1 | Step 248/695 | Loss: 0.013514367863535881\n",
      "Epoch 1 | Step 249/695 | Loss: 0.015099734999239445\n",
      "Epoch 1 | Step 250/695 | Loss: 0.013071440160274506\n",
      "Epoch 1 | Step 251/695 | Loss: 0.012508580461144447\n",
      "Epoch 1 | Step 252/695 | Loss: 0.014354107901453972\n",
      "Epoch 1 | Step 253/695 | Loss: 0.013236970640718937\n",
      "Epoch 1 | Step 254/695 | Loss: 0.01375332847237587\n",
      "Epoch 1 | Step 255/695 | Loss: 0.013988814316689968\n",
      "Epoch 1 | Step 256/695 | Loss: 0.014482616446912289\n",
      "Epoch 1 | Step 257/695 | Loss: 0.013309820555150509\n",
      "Epoch 1 | Step 258/695 | Loss: 0.014487117528915405\n",
      "Epoch 1 | Step 259/695 | Loss: 0.014779647812247276\n",
      "Epoch 1 | Step 260/695 | Loss: 0.014243003912270069\n",
      "Epoch 1 | Step 261/695 | Loss: 0.014886735938489437\n",
      "Epoch 1 | Step 262/695 | Loss: 0.01326723676174879\n",
      "Epoch 1 | Step 263/695 | Loss: 0.01421396154910326\n",
      "Epoch 1 | Step 264/695 | Loss: 0.01343695167452097\n",
      "Epoch 1 | Step 265/695 | Loss: 0.013627559877932072\n",
      "Epoch 1 | Step 266/695 | Loss: 0.012918747030198574\n",
      "Epoch 1 | Step 267/695 | Loss: 0.013969136402010918\n",
      "Epoch 1 | Step 268/695 | Loss: 0.013186871074140072\n",
      "Epoch 1 | Step 269/695 | Loss: 0.012626896612346172\n",
      "Epoch 1 | Step 270/695 | Loss: 0.014587966725230217\n",
      "Epoch 1 | Step 271/695 | Loss: 0.01374239381402731\n",
      "Epoch 1 | Step 272/695 | Loss: 0.01236889511346817\n",
      "Epoch 1 | Step 273/695 | Loss: 0.013765950687229633\n",
      "Epoch 1 | Step 274/695 | Loss: 0.01337479054927826\n",
      "Epoch 1 | Step 275/695 | Loss: 0.013486848212778568\n",
      "Epoch 1 | Step 276/695 | Loss: 0.012913709506392479\n",
      "Epoch 1 | Step 277/695 | Loss: 0.014143466018140316\n",
      "Epoch 1 | Step 278/695 | Loss: 0.012936926446855068\n",
      "Epoch 1 | Step 279/695 | Loss: 0.014063720591366291\n",
      "Epoch 1 | Step 280/695 | Loss: 0.01412174105644226\n",
      "Epoch 1 | Step 281/695 | Loss: 0.012968126684427261\n",
      "Epoch 1 | Step 282/695 | Loss: 0.01377162430435419\n",
      "Epoch 1 | Step 283/695 | Loss: 0.012942340224981308\n",
      "Epoch 1 | Step 284/695 | Loss: 0.015011723153293133\n",
      "Epoch 1 | Step 285/695 | Loss: 0.01314502488821745\n",
      "Epoch 1 | Step 286/695 | Loss: 0.013254718855023384\n",
      "Epoch 1 | Step 287/695 | Loss: 0.01352700125426054\n",
      "Epoch 1 | Step 288/695 | Loss: 0.014025021344423294\n",
      "Epoch 1 | Step 289/695 | Loss: 0.013685072772204876\n",
      "Epoch 1 | Step 290/695 | Loss: 0.013716325163841248\n",
      "Epoch 1 | Step 291/695 | Loss: 0.013913878239691257\n",
      "Epoch 1 | Step 292/695 | Loss: 0.01352075207978487\n",
      "Epoch 1 | Step 293/695 | Loss: 0.013923930935561657\n",
      "Epoch 1 | Step 294/695 | Loss: 0.013803960755467415\n",
      "Epoch 1 | Step 295/695 | Loss: 0.013029372319579124\n",
      "Epoch 1 | Step 296/695 | Loss: 0.015045993030071259\n",
      "Epoch 1 | Step 297/695 | Loss: 0.014053515158593655\n",
      "Epoch 1 | Step 298/695 | Loss: 0.011743184179067612\n",
      "Epoch 1 | Step 299/695 | Loss: 0.014281547628343105\n",
      "Epoch 1 | Step 300/695 | Loss: 0.014285052195191383\n",
      "Epoch 1 | Step 301/695 | Loss: 0.013908907771110535\n",
      "Epoch 1 | Step 302/695 | Loss: 0.015537814237177372\n",
      "Epoch 1 | Step 303/695 | Loss: 0.013670756481587887\n",
      "Epoch 1 | Step 304/695 | Loss: 0.013225947506725788\n",
      "Epoch 1 | Step 305/695 | Loss: 0.01315100584179163\n",
      "Epoch 1 | Step 306/695 | Loss: 0.012950138188898563\n",
      "Epoch 1 | Step 307/695 | Loss: 0.013721260242164135\n",
      "Epoch 1 | Step 308/695 | Loss: 0.012823914177715778\n",
      "Epoch 1 | Step 309/695 | Loss: 0.012771813198924065\n",
      "Epoch 1 | Step 310/695 | Loss: 0.014014205895364285\n",
      "Epoch 1 | Step 311/695 | Loss: 0.012683192268013954\n",
      "Epoch 1 | Step 312/695 | Loss: 0.012887370772659779\n",
      "Epoch 1 | Step 313/695 | Loss: 0.013671125285327435\n",
      "Epoch 1 | Step 314/695 | Loss: 0.01367954071611166\n",
      "Epoch 1 | Step 315/695 | Loss: 0.013793871738016605\n",
      "Epoch 1 | Step 316/695 | Loss: 0.013158788904547691\n",
      "Epoch 1 | Step 317/695 | Loss: 0.014093529433012009\n",
      "Epoch 1 | Step 318/695 | Loss: 0.014046827331185341\n",
      "Epoch 1 | Step 319/695 | Loss: 0.015216886065900326\n",
      "Epoch 1 | Step 320/695 | Loss: 0.013784973882138729\n",
      "Epoch 1 | Step 321/695 | Loss: 0.014694351702928543\n",
      "Epoch 1 | Step 322/695 | Loss: 0.014441627077758312\n",
      "Epoch 1 | Step 323/695 | Loss: 0.013900103978812695\n",
      "Epoch 1 | Step 324/695 | Loss: 0.01360482070595026\n",
      "Epoch 1 | Step 325/695 | Loss: 0.01451142504811287\n",
      "Epoch 1 | Step 326/695 | Loss: 0.014331309124827385\n",
      "Epoch 1 | Step 327/695 | Loss: 0.01355225220322609\n",
      "Epoch 1 | Step 328/695 | Loss: 0.014039848931133747\n",
      "Epoch 1 | Step 329/695 | Loss: 0.012814418412744999\n",
      "Epoch 1 | Step 330/695 | Loss: 0.013907374814152718\n",
      "Epoch 1 | Step 331/695 | Loss: 0.013755484484136105\n",
      "Epoch 1 | Step 332/695 | Loss: 0.013313449919223785\n",
      "Epoch 1 | Step 333/695 | Loss: 0.014278694987297058\n",
      "Epoch 1 | Step 334/695 | Loss: 0.012885252013802528\n",
      "Epoch 1 | Step 335/695 | Loss: 0.012484582141041756\n",
      "Epoch 1 | Step 336/695 | Loss: 0.014808435924351215\n",
      "Epoch 1 | Step 337/695 | Loss: 0.014169834554195404\n",
      "Epoch 1 | Step 338/695 | Loss: 0.013374640606343746\n",
      "Epoch 1 | Step 339/695 | Loss: 0.014199638739228249\n",
      "Epoch 1 | Step 340/695 | Loss: 0.013968952931463718\n",
      "Epoch 1 | Step 341/695 | Loss: 0.012979099527001381\n",
      "Epoch 1 | Step 342/695 | Loss: 0.012940947897732258\n",
      "Epoch 1 | Step 343/695 | Loss: 0.013198178261518478\n",
      "Epoch 1 | Step 344/695 | Loss: 0.0144876753911376\n",
      "Epoch 1 | Step 345/695 | Loss: 0.013789700344204903\n",
      "Epoch 1 | Step 346/695 | Loss: 0.013500388711690903\n",
      "Epoch 1 | Step 347/695 | Loss: 0.012420526705682278\n",
      "Epoch 1 | Step 348/695 | Loss: 0.013159657828509808\n",
      "Epoch 1 | Step 349/695 | Loss: 0.013304287567734718\n",
      "Epoch 1 | Step 350/695 | Loss: 0.012859774753451347\n",
      "Epoch 1 | Step 351/695 | Loss: 0.013483152724802494\n",
      "Epoch 1 | Step 352/695 | Loss: 0.014517984353005886\n",
      "Epoch 1 | Step 353/695 | Loss: 0.013971760869026184\n",
      "Epoch 1 | Step 354/695 | Loss: 0.012150726281106472\n",
      "Epoch 1 | Step 355/695 | Loss: 0.01327117346227169\n",
      "Epoch 1 | Step 356/695 | Loss: 0.013683751225471497\n",
      "Epoch 1 | Step 357/695 | Loss: 0.013725516386330128\n",
      "Epoch 1 | Step 358/695 | Loss: 0.014334662817418575\n",
      "Epoch 1 | Step 359/695 | Loss: 0.013391511514782906\n",
      "Epoch 1 | Step 360/695 | Loss: 0.01311702374368906\n",
      "Epoch 1 | Step 361/695 | Loss: 0.013460392132401466\n",
      "Epoch 1 | Step 362/695 | Loss: 0.014598919078707695\n",
      "Epoch 1 | Step 363/695 | Loss: 0.013019444420933723\n",
      "Epoch 1 | Step 364/695 | Loss: 0.014342102222144604\n",
      "Epoch 1 | Step 365/695 | Loss: 0.013682497665286064\n",
      "Epoch 1 | Step 366/695 | Loss: 0.014500423334538937\n",
      "Epoch 1 | Step 367/695 | Loss: 0.013082069344818592\n",
      "Epoch 1 | Step 368/695 | Loss: 0.014001747593283653\n",
      "Epoch 1 | Step 369/695 | Loss: 0.013515479862689972\n",
      "Epoch 1 | Step 370/695 | Loss: 0.012696034274995327\n",
      "Epoch 1 | Step 371/695 | Loss: 0.013113033957779408\n",
      "Epoch 1 | Step 372/695 | Loss: 0.013632839545607567\n",
      "Epoch 1 | Step 373/695 | Loss: 0.013928260654211044\n",
      "Epoch 1 | Step 374/695 | Loss: 0.013339259661734104\n",
      "Epoch 1 | Step 375/695 | Loss: 0.013522231951355934\n",
      "Epoch 1 | Step 376/695 | Loss: 0.014216728508472443\n",
      "Epoch 1 | Step 377/695 | Loss: 0.01309747714549303\n",
      "Epoch 1 | Step 378/695 | Loss: 0.012864872813224792\n",
      "Epoch 1 | Step 379/695 | Loss: 0.012388662435114384\n",
      "Epoch 1 | Step 380/695 | Loss: 0.01329308096319437\n",
      "Epoch 1 | Step 381/695 | Loss: 0.013428096659481525\n",
      "Epoch 1 | Step 382/695 | Loss: 0.012741406448185444\n",
      "Epoch 1 | Step 383/695 | Loss: 0.013782035559415817\n",
      "Epoch 1 | Step 384/695 | Loss: 0.012897688895463943\n",
      "Epoch 1 | Step 385/695 | Loss: 0.013357976451516151\n",
      "Epoch 1 | Step 386/695 | Loss: 0.012172369286417961\n",
      "Epoch 1 | Step 387/695 | Loss: 0.014308536425232887\n",
      "Epoch 1 | Step 388/695 | Loss: 0.014256415888667107\n",
      "Epoch 1 | Step 389/695 | Loss: 0.015212513506412506\n",
      "Epoch 1 | Step 390/695 | Loss: 0.013301187194883823\n",
      "Epoch 1 | Step 391/695 | Loss: 0.013001656159758568\n",
      "Epoch 1 | Step 392/695 | Loss: 0.012250486761331558\n",
      "Epoch 1 | Step 393/695 | Loss: 0.013608034700155258\n",
      "Epoch 1 | Step 394/695 | Loss: 0.01338881440460682\n",
      "Epoch 1 | Step 395/695 | Loss: 0.012465246953070164\n",
      "Epoch 1 | Step 396/695 | Loss: 0.014219517819583416\n",
      "Epoch 1 | Step 397/695 | Loss: 0.013855917379260063\n",
      "Epoch 1 | Step 398/695 | Loss: 0.01398177444934845\n",
      "Epoch 1 | Step 399/695 | Loss: 0.013190113939344883\n",
      "Epoch 1 | Step 400/695 | Loss: 0.014218523167073727\n",
      "Epoch 1 | Step 401/695 | Loss: 0.013384559191763401\n",
      "Epoch 1 | Step 402/695 | Loss: 0.014128759503364563\n",
      "Epoch 1 | Step 403/695 | Loss: 0.014128887094557285\n",
      "Epoch 1 | Step 404/695 | Loss: 0.013286193832755089\n",
      "Epoch 1 | Step 405/695 | Loss: 0.01480282936245203\n",
      "Epoch 1 | Step 406/695 | Loss: 0.012394762597978115\n",
      "Epoch 1 | Step 407/695 | Loss: 0.013096264563500881\n",
      "Epoch 1 | Step 408/695 | Loss: 0.01317478809505701\n",
      "Epoch 1 | Step 409/695 | Loss: 0.013113534078001976\n",
      "Epoch 1 | Step 410/695 | Loss: 0.012721532955765724\n",
      "Epoch 1 | Step 411/695 | Loss: 0.013547355309128761\n",
      "Epoch 1 | Step 412/695 | Loss: 0.012219204567372799\n",
      "Epoch 1 | Step 413/695 | Loss: 0.012638771906495094\n",
      "Epoch 1 | Step 414/695 | Loss: 0.013322602026164532\n",
      "Epoch 1 | Step 415/695 | Loss: 0.012253866530954838\n",
      "Epoch 1 | Step 416/695 | Loss: 0.01350371167063713\n",
      "Epoch 1 | Step 417/695 | Loss: 0.013154837302863598\n",
      "Epoch 1 | Step 418/695 | Loss: 0.013845355249941349\n",
      "Epoch 1 | Step 419/695 | Loss: 0.011989121325314045\n",
      "Epoch 1 | Step 420/695 | Loss: 0.013020527549088001\n",
      "Epoch 1 | Step 421/695 | Loss: 0.012880342081189156\n",
      "Epoch 1 | Step 422/695 | Loss: 0.01411343552172184\n",
      "Epoch 1 | Step 423/695 | Loss: 0.01304690632969141\n",
      "Epoch 1 | Step 424/695 | Loss: 0.011964711360633373\n",
      "Epoch 1 | Step 425/695 | Loss: 0.013347890228033066\n",
      "Epoch 1 | Step 426/695 | Loss: 0.013338131830096245\n",
      "Epoch 1 | Step 427/695 | Loss: 0.01274767704308033\n",
      "Epoch 1 | Step 428/695 | Loss: 0.013288102112710476\n",
      "Epoch 1 | Step 429/695 | Loss: 0.013379366137087345\n",
      "Epoch 1 | Step 430/695 | Loss: 0.01376410573720932\n",
      "Epoch 1 | Step 431/695 | Loss: 0.012481705285608768\n",
      "Epoch 1 | Step 432/695 | Loss: 0.01352665014564991\n",
      "Epoch 1 | Step 433/695 | Loss: 0.014440483413636684\n",
      "Epoch 1 | Step 434/695 | Loss: 0.013807542622089386\n",
      "Epoch 1 | Step 435/695 | Loss: 0.012828296050429344\n",
      "Epoch 1 | Step 436/695 | Loss: 0.012550986371934414\n",
      "Epoch 1 | Step 437/695 | Loss: 0.013032586313784122\n",
      "Epoch 1 | Step 438/695 | Loss: 0.013230153359472752\n",
      "Epoch 1 | Step 439/695 | Loss: 0.01368042267858982\n",
      "Epoch 1 | Step 440/695 | Loss: 0.014471212401986122\n",
      "Epoch 1 | Step 441/695 | Loss: 0.012491745874285698\n",
      "Epoch 1 | Step 442/695 | Loss: 0.013995976187288761\n",
      "Epoch 1 | Step 443/695 | Loss: 0.014281640760600567\n",
      "Epoch 1 | Step 444/695 | Loss: 0.013778211548924446\n",
      "Epoch 1 | Step 445/695 | Loss: 0.014136300422251225\n",
      "Epoch 1 | Step 446/695 | Loss: 0.01377446111291647\n",
      "Epoch 1 | Step 447/695 | Loss: 0.013559339568018913\n",
      "Epoch 1 | Step 448/695 | Loss: 0.01319403201341629\n",
      "Epoch 1 | Step 449/695 | Loss: 0.013676982372999191\n",
      "Epoch 1 | Step 450/695 | Loss: 0.014220502227544785\n",
      "Epoch 1 | Step 451/695 | Loss: 0.013143794611096382\n",
      "Epoch 1 | Step 452/695 | Loss: 0.01387669425457716\n",
      "Epoch 1 | Step 453/695 | Loss: 0.01347657386213541\n",
      "Epoch 1 | Step 454/695 | Loss: 0.013253342360258102\n",
      "Epoch 1 | Step 455/695 | Loss: 0.014764546416699886\n",
      "Epoch 1 | Step 456/695 | Loss: 0.015242336317896843\n",
      "Epoch 1 | Step 457/695 | Loss: 0.013025063090026379\n",
      "Epoch 1 | Step 458/695 | Loss: 0.013592355884611607\n",
      "Epoch 1 | Step 459/695 | Loss: 0.012358760461211205\n",
      "Epoch 1 | Step 460/695 | Loss: 0.014373508282005787\n",
      "Epoch 1 | Step 461/695 | Loss: 0.014193390496075153\n",
      "Epoch 1 | Step 462/695 | Loss: 0.012987016700208187\n",
      "Epoch 1 | Step 463/695 | Loss: 0.012960100546479225\n",
      "Epoch 1 | Step 464/695 | Loss: 0.012747571803629398\n",
      "Epoch 1 | Step 465/695 | Loss: 0.014462379738688469\n",
      "Epoch 1 | Step 466/695 | Loss: 0.012946105562150478\n",
      "Epoch 1 | Step 467/695 | Loss: 0.013621076010167599\n",
      "Epoch 1 | Step 468/695 | Loss: 0.01155730988830328\n",
      "Epoch 1 | Step 469/695 | Loss: 0.01218970026820898\n",
      "Epoch 1 | Step 470/695 | Loss: 0.014679891988635063\n",
      "Epoch 1 | Step 471/695 | Loss: 0.013067123480141163\n",
      "Epoch 1 | Step 472/695 | Loss: 0.014358626678586006\n",
      "Epoch 1 | Step 473/695 | Loss: 0.013074135407805443\n",
      "Epoch 1 | Step 474/695 | Loss: 0.011619635857641697\n",
      "Epoch 1 | Step 475/695 | Loss: 0.014388805255293846\n",
      "Epoch 1 | Step 476/695 | Loss: 0.01369450893253088\n",
      "Epoch 1 | Step 477/695 | Loss: 0.012637421488761902\n",
      "Epoch 1 | Step 478/695 | Loss: 0.014945169910788536\n",
      "Epoch 1 | Step 479/695 | Loss: 0.013661521486938\n",
      "Epoch 1 | Step 480/695 | Loss: 0.012043211609125137\n",
      "Epoch 1 | Step 481/695 | Loss: 0.013836483471095562\n",
      "Epoch 1 | Step 482/695 | Loss: 0.013745354488492012\n",
      "Epoch 1 | Step 483/695 | Loss: 0.01321402471512556\n",
      "Epoch 1 | Step 484/695 | Loss: 0.013887583278119564\n",
      "Epoch 1 | Step 485/695 | Loss: 0.013547902926802635\n",
      "Epoch 1 | Step 486/695 | Loss: 0.01409552339464426\n",
      "Epoch 1 | Step 487/695 | Loss: 0.013094592839479446\n",
      "Epoch 1 | Step 488/695 | Loss: 0.013144395314157009\n",
      "Epoch 1 | Step 489/695 | Loss: 0.013966030441224575\n",
      "Epoch 1 | Step 490/695 | Loss: 0.01421018224209547\n",
      "Epoch 1 | Step 491/695 | Loss: 0.013173939660191536\n",
      "Epoch 1 | Step 492/695 | Loss: 0.013910219073295593\n",
      "Epoch 1 | Step 493/695 | Loss: 0.013387718237936497\n",
      "Epoch 1 | Step 494/695 | Loss: 0.014731117524206638\n",
      "Epoch 1 | Step 495/695 | Loss: 0.011914044618606567\n",
      "Epoch 1 | Step 496/695 | Loss: 0.01276105921715498\n",
      "Epoch 1 | Step 497/695 | Loss: 0.011961149983108044\n",
      "Epoch 1 | Step 498/695 | Loss: 0.01284151989966631\n",
      "Epoch 1 | Step 499/695 | Loss: 0.012452325783669949\n",
      "Epoch 1 | Step 500/695 | Loss: 0.012671551667153835\n",
      "Epoch 1 | Step 501/695 | Loss: 0.013129141181707382\n",
      "Epoch 1 | Step 502/695 | Loss: 0.012163259088993073\n",
      "Epoch 1 | Step 503/695 | Loss: 0.013886223547160625\n",
      "Epoch 1 | Step 504/695 | Loss: 0.013485865667462349\n",
      "Epoch 1 | Step 505/695 | Loss: 0.012670797295868397\n",
      "Epoch 1 | Step 506/695 | Loss: 0.012225165963172913\n",
      "Epoch 1 | Step 507/695 | Loss: 0.012601232156157494\n",
      "Epoch 1 | Step 508/695 | Loss: 0.013053636997938156\n",
      "Epoch 1 | Step 509/695 | Loss: 0.013544891960918903\n",
      "Epoch 1 | Step 510/695 | Loss: 0.013353256508708\n",
      "Epoch 1 | Step 511/695 | Loss: 0.01256166584789753\n",
      "Epoch 1 | Step 512/695 | Loss: 0.012291326187551022\n",
      "Epoch 1 | Step 513/695 | Loss: 0.011588522233068943\n",
      "Epoch 1 | Step 514/695 | Loss: 0.01197595614939928\n",
      "Epoch 1 | Step 515/695 | Loss: 0.013310281559824944\n",
      "Epoch 1 | Step 516/695 | Loss: 0.01351876650005579\n",
      "Epoch 1 | Step 517/695 | Loss: 0.011916113086044788\n",
      "Epoch 1 | Step 518/695 | Loss: 0.013078637421131134\n",
      "Epoch 1 | Step 519/695 | Loss: 0.012562454678118229\n",
      "Epoch 1 | Step 520/695 | Loss: 0.013056686148047447\n",
      "Epoch 1 | Step 521/695 | Loss: 0.012350228615105152\n",
      "Epoch 1 | Step 522/695 | Loss: 0.013027764856815338\n",
      "Epoch 1 | Step 523/695 | Loss: 0.01245648879557848\n",
      "Epoch 1 | Step 524/695 | Loss: 0.014967311173677444\n",
      "Epoch 1 | Step 525/695 | Loss: 0.01177241001278162\n",
      "Epoch 1 | Step 526/695 | Loss: 0.01388038881123066\n",
      "Epoch 1 | Step 527/695 | Loss: 0.011864953674376011\n",
      "Epoch 1 | Step 528/695 | Loss: 0.01303043682128191\n",
      "Epoch 1 | Step 529/695 | Loss: 0.01497038546949625\n",
      "Epoch 1 | Step 530/695 | Loss: 0.01250559464097023\n",
      "Epoch 1 | Step 531/695 | Loss: 0.012917453423142433\n",
      "Epoch 1 | Step 532/695 | Loss: 0.013153991661965847\n",
      "Epoch 1 | Step 533/695 | Loss: 0.012839886359870434\n",
      "Epoch 1 | Step 534/695 | Loss: 0.012648346833884716\n",
      "Epoch 1 | Step 535/695 | Loss: 0.012843005359172821\n",
      "Epoch 1 | Step 536/695 | Loss: 0.012678369879722595\n",
      "Epoch 1 | Step 537/695 | Loss: 0.013769046403467655\n",
      "Epoch 1 | Step 538/695 | Loss: 0.01274807471781969\n",
      "Epoch 1 | Step 539/695 | Loss: 0.01291652861982584\n",
      "Epoch 1 | Step 540/695 | Loss: 0.012471228837966919\n",
      "Epoch 1 | Step 541/695 | Loss: 0.012416076846420765\n",
      "Epoch 1 | Step 542/695 | Loss: 0.013779565691947937\n",
      "Epoch 1 | Step 543/695 | Loss: 0.013016873970627785\n",
      "Epoch 1 | Step 544/695 | Loss: 0.013107500039041042\n",
      "Epoch 1 | Step 545/695 | Loss: 0.013476874679327011\n",
      "Epoch 1 | Step 546/695 | Loss: 0.012943953275680542\n",
      "Epoch 1 | Step 547/695 | Loss: 0.012504004873335361\n",
      "Epoch 1 | Step 548/695 | Loss: 0.012681170366704464\n",
      "Epoch 1 | Step 549/695 | Loss: 0.011868486180901527\n",
      "Epoch 1 | Step 550/695 | Loss: 0.012707959860563278\n",
      "Epoch 1 | Step 551/695 | Loss: 0.013190985657274723\n",
      "Epoch 1 | Step 552/695 | Loss: 0.01339507382363081\n",
      "Epoch 1 | Step 553/695 | Loss: 0.012130407616496086\n",
      "Epoch 1 | Step 554/695 | Loss: 0.012843647040426731\n",
      "Epoch 1 | Step 555/695 | Loss: 0.013464185409247875\n",
      "Epoch 1 | Step 556/695 | Loss: 0.012663507834076881\n",
      "Epoch 1 | Step 557/695 | Loss: 0.012396632693707943\n",
      "Epoch 1 | Step 558/695 | Loss: 0.012895461171865463\n",
      "Epoch 1 | Step 559/695 | Loss: 0.013631624169647694\n",
      "Epoch 1 | Step 560/695 | Loss: 0.012589380145072937\n",
      "Epoch 1 | Step 561/695 | Loss: 0.013282879255712032\n",
      "Epoch 1 | Step 562/695 | Loss: 0.012491568922996521\n",
      "Epoch 1 | Step 563/695 | Loss: 0.014160986989736557\n",
      "Epoch 1 | Step 564/695 | Loss: 0.013146737590432167\n",
      "Epoch 1 | Step 565/695 | Loss: 0.01213437132537365\n",
      "Epoch 1 | Step 566/695 | Loss: 0.012697350233793259\n",
      "Epoch 1 | Step 567/695 | Loss: 0.012991586700081825\n",
      "Epoch 1 | Step 568/695 | Loss: 0.011603289283812046\n",
      "Epoch 1 | Step 569/695 | Loss: 0.012629061937332153\n",
      "Epoch 1 | Step 570/695 | Loss: 0.012337976135313511\n",
      "Epoch 1 | Step 571/695 | Loss: 0.0135688167065382\n",
      "Epoch 1 | Step 572/695 | Loss: 0.012964808382093906\n",
      "Epoch 1 | Step 573/695 | Loss: 0.011627377942204475\n",
      "Epoch 1 | Step 574/695 | Loss: 0.012630787678062916\n",
      "Epoch 1 | Step 575/695 | Loss: 0.012475554831326008\n",
      "Epoch 1 | Step 576/695 | Loss: 0.012031573802232742\n",
      "Epoch 1 | Step 577/695 | Loss: 0.012614304199814796\n",
      "Epoch 1 | Step 578/695 | Loss: 0.012757220305502415\n",
      "Epoch 1 | Step 579/695 | Loss: 0.012417010962963104\n",
      "Epoch 1 | Step 580/695 | Loss: 0.013840987347066402\n",
      "Epoch 1 | Step 581/695 | Loss: 0.012712480500340462\n",
      "Epoch 1 | Step 582/695 | Loss: 0.012159767560660839\n",
      "Epoch 1 | Step 583/695 | Loss: 0.012935821898281574\n",
      "Epoch 1 | Step 584/695 | Loss: 0.013980305753648281\n",
      "Epoch 1 | Step 585/695 | Loss: 0.01300610788166523\n",
      "Epoch 1 | Step 586/695 | Loss: 0.011887690983712673\n",
      "Epoch 1 | Step 587/695 | Loss: 0.012410924769937992\n",
      "Epoch 1 | Step 588/695 | Loss: 0.012454207986593246\n",
      "Epoch 1 | Step 589/695 | Loss: 0.012993033975362778\n",
      "Epoch 1 | Step 590/695 | Loss: 0.011333377100527287\n",
      "Epoch 1 | Step 591/695 | Loss: 0.011825342662632465\n",
      "Epoch 1 | Step 592/695 | Loss: 0.014266153797507286\n",
      "Epoch 1 | Step 593/695 | Loss: 0.011938604526221752\n",
      "Epoch 1 | Step 594/695 | Loss: 0.01203890424221754\n",
      "Epoch 1 | Step 595/695 | Loss: 0.013253678567707539\n",
      "Epoch 1 | Step 596/695 | Loss: 0.013419038616120815\n",
      "Epoch 1 | Step 597/695 | Loss: 0.012515697628259659\n",
      "Epoch 1 | Step 598/695 | Loss: 0.013558147475123405\n",
      "Epoch 1 | Step 599/695 | Loss: 0.01368897408246994\n",
      "Epoch 1 | Step 600/695 | Loss: 0.013779059052467346\n",
      "Epoch 1 | Step 601/695 | Loss: 0.011846933513879776\n",
      "Train HACCARD: 0.1316413860006046 | Train F1 Micro: 0.2236643229321933 | Val HACCARD: 0.13191789971239426 | Val F1 Micro: 0.22410449002575544\n",
      "Epoch 1 | Step 602/695 | Loss: 0.01403757929801941\n",
      "Epoch 1 | Step 603/695 | Loss: 0.013120882213115692\n",
      "Epoch 1 | Step 604/695 | Loss: 0.012931187637150288\n",
      "Epoch 1 | Step 605/695 | Loss: 0.011891325935721397\n",
      "Epoch 1 | Step 606/695 | Loss: 0.012781654484570026\n",
      "Epoch 1 | Step 607/695 | Loss: 0.013405057601630688\n",
      "Epoch 1 | Step 608/695 | Loss: 0.011668161489069462\n",
      "Epoch 1 | Step 609/695 | Loss: 0.012157143093645573\n",
      "Epoch 1 | Step 610/695 | Loss: 0.012790706939995289\n",
      "Epoch 1 | Step 611/695 | Loss: 0.011981717310845852\n",
      "Epoch 1 | Step 612/695 | Loss: 0.011708549223840237\n",
      "Epoch 1 | Step 613/695 | Loss: 0.012556297704577446\n",
      "Epoch 1 | Step 614/695 | Loss: 0.012642654590308666\n",
      "Epoch 1 | Step 615/695 | Loss: 0.012687712907791138\n",
      "Epoch 1 | Step 616/695 | Loss: 0.011756647378206253\n",
      "Epoch 1 | Step 617/695 | Loss: 0.013466455973684788\n",
      "Epoch 1 | Step 618/695 | Loss: 0.01357264444231987\n",
      "Epoch 1 | Step 619/695 | Loss: 0.0144095029681921\n",
      "Epoch 1 | Step 620/695 | Loss: 0.013196236453950405\n",
      "Epoch 1 | Step 621/695 | Loss: 0.012076258659362793\n",
      "Epoch 1 | Step 622/695 | Loss: 0.01321771927177906\n",
      "Epoch 1 | Step 623/695 | Loss: 0.01256671641021967\n",
      "Epoch 1 | Step 624/695 | Loss: 0.012708636000752449\n",
      "Epoch 1 | Step 625/695 | Loss: 0.012857409194111824\n",
      "Epoch 1 | Step 626/695 | Loss: 0.013154029846191406\n",
      "Epoch 1 | Step 627/695 | Loss: 0.01325106993317604\n",
      "Epoch 1 | Step 628/695 | Loss: 0.013540705665946007\n",
      "Epoch 1 | Step 629/695 | Loss: 0.01374540664255619\n",
      "Epoch 1 | Step 630/695 | Loss: 0.012274440377950668\n",
      "Epoch 1 | Step 631/695 | Loss: 0.012824167497456074\n",
      "Epoch 1 | Step 632/695 | Loss: 0.013357422314584255\n",
      "Epoch 1 | Step 633/695 | Loss: 0.013217952102422714\n",
      "Epoch 1 | Step 634/695 | Loss: 0.012169581837952137\n",
      "Epoch 1 | Step 635/695 | Loss: 0.013007156550884247\n",
      "Epoch 1 | Step 636/695 | Loss: 0.012324023060500622\n",
      "Epoch 1 | Step 637/695 | Loss: 0.013919898308813572\n",
      "Epoch 1 | Step 638/695 | Loss: 0.012337567284703255\n",
      "Epoch 1 | Step 639/695 | Loss: 0.012410326860845089\n",
      "Epoch 1 | Step 640/695 | Loss: 0.014041082002222538\n",
      "Epoch 1 | Step 641/695 | Loss: 0.014198096469044685\n",
      "Epoch 1 | Step 642/695 | Loss: 0.014576724730432034\n",
      "Epoch 1 | Step 643/695 | Loss: 0.013090787455439568\n",
      "Epoch 1 | Step 644/695 | Loss: 0.011374480091035366\n",
      "Epoch 1 | Step 645/695 | Loss: 0.012327859178185463\n",
      "Epoch 1 | Step 646/695 | Loss: 0.012408840470016003\n",
      "Epoch 1 | Step 647/695 | Loss: 0.012891341932117939\n",
      "Epoch 1 | Step 648/695 | Loss: 0.012722344137728214\n",
      "Epoch 1 | Step 649/695 | Loss: 0.013102884404361248\n",
      "Epoch 1 | Step 650/695 | Loss: 0.013176819309592247\n",
      "Epoch 1 | Step 651/695 | Loss: 0.012062347494065762\n",
      "Epoch 1 | Step 652/695 | Loss: 0.013104943558573723\n",
      "Epoch 1 | Step 653/695 | Loss: 0.01297184731811285\n",
      "Epoch 1 | Step 654/695 | Loss: 0.014367390424013138\n",
      "Epoch 1 | Step 655/695 | Loss: 0.01262209378182888\n",
      "Epoch 1 | Step 656/695 | Loss: 0.012424493208527565\n",
      "Epoch 1 | Step 657/695 | Loss: 0.012459801509976387\n",
      "Epoch 1 | Step 658/695 | Loss: 0.01238937396556139\n",
      "Epoch 1 | Step 659/695 | Loss: 0.013164608739316463\n",
      "Epoch 1 | Step 660/695 | Loss: 0.012161223217844963\n",
      "Epoch 1 | Step 661/695 | Loss: 0.012776421383023262\n",
      "Epoch 1 | Step 662/695 | Loss: 0.012968262657523155\n",
      "Epoch 1 | Step 663/695 | Loss: 0.013155573047697544\n",
      "Epoch 1 | Step 664/695 | Loss: 0.012629934586584568\n",
      "Epoch 1 | Step 665/695 | Loss: 0.012270091101527214\n",
      "Epoch 1 | Step 666/695 | Loss: 0.013075746595859528\n",
      "Epoch 1 | Step 667/695 | Loss: 0.013475995510816574\n",
      "Epoch 1 | Step 668/695 | Loss: 0.012712547555565834\n",
      "Epoch 1 | Step 669/695 | Loss: 0.01249666791409254\n",
      "Epoch 1 | Step 670/695 | Loss: 0.013280010782182217\n",
      "Epoch 1 | Step 671/695 | Loss: 0.013043301180005074\n",
      "Epoch 1 | Step 672/695 | Loss: 0.011793015524744987\n",
      "Epoch 1 | Step 673/695 | Loss: 0.01243808027356863\n",
      "Epoch 1 | Step 674/695 | Loss: 0.013184025883674622\n",
      "Epoch 1 | Step 675/695 | Loss: 0.01303380448371172\n",
      "Epoch 1 | Step 676/695 | Loss: 0.01342863030731678\n",
      "Epoch 1 | Step 677/695 | Loss: 0.01283490750938654\n",
      "Epoch 1 | Step 678/695 | Loss: 0.011553170159459114\n",
      "Epoch 1 | Step 679/695 | Loss: 0.012484708800911903\n",
      "Epoch 1 | Step 680/695 | Loss: 0.013093718327581882\n",
      "Epoch 1 | Step 681/695 | Loss: 0.013844129629433155\n",
      "Epoch 1 | Step 682/695 | Loss: 0.012387233786284924\n",
      "Epoch 1 | Step 683/695 | Loss: 0.013986503705382347\n",
      "Epoch 1 | Step 684/695 | Loss: 0.012797189876437187\n",
      "Epoch 1 | Step 685/695 | Loss: 0.012170939706265926\n",
      "Epoch 1 | Step 686/695 | Loss: 0.012433517724275589\n",
      "Epoch 1 | Step 687/695 | Loss: 0.011682499200105667\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 128\n",
    "eval_each_n_steps = 600\n",
    "for epoch in range(n_epochs):\n",
    "    for step in range(len(X_train_split) // batch_size):\n",
    "        batch_X = X_train_split[step * batch_size:(step + 1) * batch_size]\n",
    "        batch_y = torch.tensor(y_train_multi_hot_encoded_split[step * batch_size:(step + 1) * batch_size], dtype=torch.float32).to(device)\n",
    "\n",
    "        logits = mmm(batch_X)\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} | Step {step + 1}/{len(X_train) // batch_size} | Loss: {loss.item()}\")\n",
    "        if step % eval_each_n_steps == 0:\n",
    "            mmm.eval()\n",
    "            train_preds = make_predictions(mmm, X_train_split[:2000])\n",
    "            train_haccard = eval_haccard(train_preds, y_train_split[:2000])\n",
    "            train_f1_micro = eval_f1_micro(train_preds, y_train_split[:2000])\n",
    "\n",
    "            val_preds = make_predictions(mmm, X_val_split)\n",
    "            val_haccard = eval_haccard(val_preds, y_val_split)\n",
    "            val_f1_micro = eval_f1_micro(val_preds, y_val_split)\n",
    "            print(f\"Train HACCARD: {train_haccard} | Train F1 Micro: {train_f1_micro} | Val HACCARD: {val_haccard} | Val F1 Micro: {val_f1_micro}\")\n",
    "            mmm.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d8436",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52946cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = make_predictions(mmm, X_test)\n",
    "\n",
    "lengths = [\n",
    "    len(xx) for xx in test_predictions\n",
    "]\n",
    "print(\"Median:\", np.median(lengths))\n",
    "print(\"Mean:\", np.mean(lengths))\n",
    "print(\"Max:\", np.max(lengths))\n",
    "print(\"Min:\", np.min(lengths))\n",
    "\n",
    "\n",
    "x_test_predictions_list_str = [\n",
    "    \" \".join(map(str, x))\n",
    "    for x in test_predictions\n",
    "]\n",
    "\n",
    "submission_dataframe = pd.DataFrame({\n",
    "    \"surveyId\": test_data.surveyId,\n",
    "    \"predictions\": x_test_predictions_list_str\n",
    "})\n",
    "\n",
    "submission_dataframe.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89b189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
